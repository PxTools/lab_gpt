{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kjøre modellen lokalt\n",
    "### Laste inn modellen fra Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 662/662 [00:00<00:00, 6.69MB/s]\n",
      "model.safetensors: 100%|██████████| 3.13G/3.13G [05:02<00:00, 10.4MB/s]\n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 778kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, T5ForConditionalGeneration\n",
    "\n",
    "# Funerer også med \"AutoModelForSeq2SeqLM\"\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "#  5 min 9 sek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 30.2MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 9.44MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 26.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 4.18MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symphony\n"
     ]
    }
   ],
   "source": [
    "# input_to_model = \"A skier slides down a frictionless slope of height 40m and length 80m. What's the skier's speed at the bottom?\"\n",
    "input_to_model = \"The answer to the universe is\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie alte sind Sie?\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"translate English to German: How old are you?\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Det er en til\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"translate English to Norwegian: How old are you?\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where did you get yours?\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"translate Norwegian to English: Hvor gammel er du?\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sexy sex\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"?\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_new_tokens=50  -  43s\n",
    "\n",
    "A skier slides down a frictionless slope of height 40m and length 80m. What's the skier's speed at the bottom?\n",
    "\n",
    "Solution:\n",
    "The skier's speed at the bottom is the same as the speed of a freely falling object with a height of 40m. Using the formula v^2 = u^2 + 2gh, where u is the initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norsk fungerer svært dårlig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonsai trr\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"Simen har to bonsai trær som er av type ficus. Han har også en orkide som heter Ola. Hvor mange trær har Simen?\"\n",
    "res = pipe(input_to_model, max_new_tokens=200)\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"Simen has two bonsai trees of the ficus type. He also has an oak named Ola. How many trees does Simen have?\"\n",
    "res = pipe(input_to_model, max_new_tokens=200)\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikke helt optimalt :////"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
