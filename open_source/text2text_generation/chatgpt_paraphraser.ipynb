{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kjøre modellen lokalt\n",
    "### Laste inn modellen fra Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 1.61k/1.61k [00:00<00:00, 6.84MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 892M/892M [01:18<00:00, 11.4MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 114kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForSeq2SeqLM, pipeline, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# 1m 22s\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 23.7MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 9.07MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 12.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mystery of the universe is solved by: \"The answer to the universe is...\"\n"
     ]
    }
   ],
   "source": [
    "# input_to_model = \"A skier slides down a frictionless slope of height 40m and length 80m. What's the skier's speed at the bottom?\"\n",
    "input_to_model = \"The answer to the universe is\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What are some must-see places in New York?',\n",
       " 'Can you suggest some must-see spots in New York?',\n",
       " 'Where should one go to experience the best NYC has to offer?',\n",
       " 'Which places should I visit in New York?',\n",
       " 'What are the top destinations to explore in New York?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'What are the best places to see in New York?'\n",
    "paraphrase(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her blir den ganske forvirret, og klarer ikke helt å paraphrase korrekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you tell me your age in terms of English?',\n",
       " 'How old are you?',\n",
       " 'What is your age in terms of English?',\n",
       " 'In terms of years, how old are you?',\n",
       " 'Could you share your age in English?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'translate English to German: How old are you?'\n",
    "paraphrase(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klarer ikke Norsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hei, kan du behöver du till finne salt.',\n",
       " 'HEI: \"Can you please let me know how I\\'m faring?...\"',\n",
       " '\"Hei, kan du vill till tong til att du jag varfördighed?\"',\n",
       " 'I sprit veya om du att du du gib du till det kommer piget finne?',\n",
       " 'Hr du fortuit p parin s kommer att du fre?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hei, kan du hjelpe meg med å finne salt'\n",
    "paraphrase(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
