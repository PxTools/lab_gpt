{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kjøre modellen lokalt\n",
    "### Laste inn modellen fra Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\")\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are a lot of places to see in New York.',\n",
       " 'There are many places to see in New York.',\n",
       " 'What are the best places to visit in New York?',\n",
       " 'There are many places to visit in New York.',\n",
       " 'What are some of the best places to visit in New York?',\n",
       " 'What are some of the best places in New York?',\n",
       " 'What are the best places to visit New York?',\n",
       " 'What are some of the best places to visit New York?',\n",
       " 'The best places to visit in New York.',\n",
       " 'What are the best places to see New York?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "# context = \"The ultimate test of your knowledge is your capacity to convey it to another.\"\n",
    "context = \"What are the best places to see in New York\"\n",
    "get_response(context,num_return_sequences,num_beams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['How old are you?',\n",
       " 'Do you know how old you are?',\n",
       " 'Is it possible to translate English to German?',\n",
       " 'What is the age of you in English to German?',\n",
       " 'Is it possible to translate English to German.',\n",
       " 'How old do you want to be?',\n",
       " 'Is it possible to translate the English to German?',\n",
       " 'How old are you in English?',\n",
       " 'What is the age of you?',\n",
       " 'What age are you?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"translate English to German: How old are you?\"\n",
    "get_response(context,num_return_sequences,num_beams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klarer ikke Norsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I New York, Hvor er de beste.',\n",
       " 'I New York, har de beste stedene.',\n",
       " 'I New York, har de beste.',\n",
       " 'I New York har de beste.',\n",
       " 'De beste stedene is in New York.',\n",
       " 'I New York, Hvorer de beste.',\n",
       " 'I New York er de beste.',\n",
       " 'I New York, Hvorer de beste stedene.',\n",
       " 'I New York, Hvor er de beste stedene.',\n",
       " 'I New York har beste.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"Hvor er de beste stedene å bo i New York\"\n",
    "get_response(context,num_return_sequences,num_beams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
